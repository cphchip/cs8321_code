{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs8321 Lab 2 - Transfer Learning and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chip Henderson - 48996654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0 points] Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the classification task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the feature data? Who collected the data? Why? When? Is it multimodal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What evaluation criteria will you be using, why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0 points] Describe the foundational model that you will be using to transfer learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What tasks was the foundational model trained from?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the hugging face [blog site](https://huggingface.co/google-bert/bert-base-uncased?text=The+goal+of+a+dog%27s+life+is+%5BMASK%5D), \"the BERT model was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia. It is also known as the Toronto Book Corpus, and consists of the text of around 7,000 self-published books scraped from the indie ebook distribution website Smashwords [per wikipedia](https://en.wikipedia.org/wiki/BookCorpus). The dataset consists of around 985 million words across a large span of genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain if the new task is within the same domain, across domains, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.0 points] Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Source: modified from https://nijianmo.github.io/amazon/index.html for importing data\n",
    "# Customized path and df name \n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "path = \"../Data_sources/Electronics_5.json.gz\"\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "      <td>09 18, 1999</td>\n",
       "      <td>AAP7PPBU72QFM</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>D. C. Carrad</td>\n",
       "      <td>This is the best novel I have read in 2 or 3 y...</td>\n",
       "      <td>A star is born</td>\n",
       "      <td>937612800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 23, 2013</td>\n",
       "      <td>A2E168DTVGE6SV</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>Evy</td>\n",
       "      <td>Pages and pages of introspection, in the style...</td>\n",
       "      <td>A stream of consciousness novel</td>\n",
       "      <td>1382486400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>09 2, 2008</td>\n",
       "      <td>A1ER5AYS3FQ9O3</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Kcorn</td>\n",
       "      <td>This is the kind of novel to read when you hav...</td>\n",
       "      <td>I'm a huge fan of the author and this one did ...</td>\n",
       "      <td>1220313600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>09 4, 2000</td>\n",
       "      <td>A1T17LMQABMBN5</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Caf Girl Writes</td>\n",
       "      <td>What gorgeous language! What an incredible wri...</td>\n",
       "      <td>The most beautiful book I have ever read!</td>\n",
       "      <td>968025600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>02 4, 2000</td>\n",
       "      <td>A3QHJ0FXK33OBE</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>W. Shane Schmidt</td>\n",
       "      <td>I was taken in by reviews that compared this b...</td>\n",
       "      <td>A dissenting view--In part.</td>\n",
       "      <td>949622400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>06 5, 2013</td>\n",
       "      <td>A3IYSOTP3HA77N</td>\n",
       "      <td>0380709473</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>B. Marks</td>\n",
       "      <td>I read this probably 50 years ago in my youth ...</td>\n",
       "      <td>Above average mystery</td>\n",
       "      <td>1370390400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>06 27, 2016</td>\n",
       "      <td>A11SXV34PZUQ5E</td>\n",
       "      <td>0380709473</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>Tom C.</td>\n",
       "      <td>I read every Perry mason book voraciously. Fin...</td>\n",
       "      <td>Lam is cool!</td>\n",
       "      <td>1466985600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>07 30, 2015</td>\n",
       "      <td>A2AUQM1HT2D5T8</td>\n",
       "      <td>0380709473</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>ema</td>\n",
       "      <td>I love this series of Bertha and Lamb..  Great...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1438214400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>02 16, 2015</td>\n",
       "      <td>A3UD8JRWLX6SRX</td>\n",
       "      <td>0380709473</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Michael O.</td>\n",
       "      <td>Great read!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1424044800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>11 21, 2013</td>\n",
       "      <td>A3MV1KKHX51FYT</td>\n",
       "      <td>0380709473</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Acute Observer</td>\n",
       "      <td>Crows Can't Count, A.A. Fair\\n\\nMr. Harry Shar...</td>\n",
       "      <td>A Fast and Far Moving Adventure</td>\n",
       "      <td>1384992000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0   67      True  09 18, 1999   AAP7PPBU72QFM  0151004714   \n",
       "1      3.0    5      True  10 23, 2013  A2E168DTVGE6SV  0151004714   \n",
       "2      5.0    4     False   09 2, 2008  A1ER5AYS3FQ9O3  0151004714   \n",
       "3      5.0   13     False   09 4, 2000  A1T17LMQABMBN5  0151004714   \n",
       "4      3.0    8      True   02 4, 2000  A3QHJ0FXK33OBE  0151004714   \n",
       "5      4.0  NaN      True   06 5, 2013  A3IYSOTP3HA77N  0380709473   \n",
       "6      5.0  NaN      True  06 27, 2016  A11SXV34PZUQ5E  0380709473   \n",
       "7      5.0  NaN      True  07 30, 2015  A2AUQM1HT2D5T8  0380709473   \n",
       "8      5.0  NaN      True  02 16, 2015  A3UD8JRWLX6SRX  0380709473   \n",
       "9      4.0  NaN     False  11 21, 2013  A3MV1KKHX51FYT  0380709473   \n",
       "\n",
       "                            style      reviewerName  \\\n",
       "0       {'Format:': ' Hardcover'}      D. C. Carrad   \n",
       "1  {'Format:': ' Kindle Edition'}               Evy   \n",
       "2       {'Format:': ' Paperback'}             Kcorn   \n",
       "3       {'Format:': ' Hardcover'}   Caf Girl Writes   \n",
       "4       {'Format:': ' Hardcover'}  W. Shane Schmidt   \n",
       "5  {'Format:': ' Kindle Edition'}          B. Marks   \n",
       "6  {'Format:': ' Kindle Edition'}            Tom C.   \n",
       "7  {'Format:': ' Kindle Edition'}               ema   \n",
       "8       {'Format:': ' Paperback'}        Michael O.   \n",
       "9       {'Format:': ' Paperback'}    Acute Observer   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This is the best novel I have read in 2 or 3 y...   \n",
       "1  Pages and pages of introspection, in the style...   \n",
       "2  This is the kind of novel to read when you hav...   \n",
       "3  What gorgeous language! What an incredible wri...   \n",
       "4  I was taken in by reviews that compared this b...   \n",
       "5  I read this probably 50 years ago in my youth ...   \n",
       "6  I read every Perry mason book voraciously. Fin...   \n",
       "7  I love this series of Bertha and Lamb..  Great...   \n",
       "8                                        Great read!   \n",
       "9  Crows Can't Count, A.A. Fair\\n\\nMr. Harry Shar...   \n",
       "\n",
       "                                             summary  unixReviewTime image  \n",
       "0                                     A star is born       937612800   NaN  \n",
       "1                    A stream of consciousness novel      1382486400   NaN  \n",
       "2  I'm a huge fan of the author and this one did ...      1220313600   NaN  \n",
       "3          The most beautiful book I have ever read!       968025600   NaN  \n",
       "4                        A dissenting view--In part.       949622400   NaN  \n",
       "5                              Above average mystery      1370390400   NaN  \n",
       "6                                       Lam is cool!      1466985600   NaN  \n",
       "7                                         Five Stars      1438214400   NaN  \n",
       "8                                         Five Stars      1424044800   NaN  \n",
       "9                    A Fast and Far Moving Adventure      1384992000   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, these don't all seem like electronics reviews...but for my purposes it really doesn't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explain how you performed this operation and why you think it is reasonable to split this particular dataset this way**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For multi-task datasets, be sure to explain if it is appropriate to stratify within each task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the dataset is already split for you, explain how the split was achieved and how it is stratified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0 points] Train a model from scratch to perform the classification task (this does NOT need to be a transformer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify the model converges (even if the model is overfit).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network 1 (CNN-1)\n",
    "\n",
    "The first CNN I'll run will consist of 64 filters with a width of 5. I'm changing the filter size from the in-class example because my dataset is quite a bit smaller. So my thought being I won't need so many filters to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Source: Modified from in-class lecture, cs7324, notebook 13a\n",
    "    from tensorflow.keras.metrics import Precision\n",
    "\n",
    "    EMBED_SIZE = 50  # same size as loaded from GLOVE\n",
    "    NUM_CLASSES = 3\n",
    "    sequence_input = Input(shape=(MAX_TWEET_LEN,), dtype='int32')\n",
    "    # starting size: 500\n",
    "    embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "    x = Conv1D(64, 5, activation='relu',\n",
    "            kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "    # after conv, size becomes: 500-4=496\n",
    "    x = MaxPooling1D(5)(x) # after max pool, 996/5 = 99\n",
    "    x = Dropout(0.2)(x) # after dropout, size is 95\n",
    "    x = Conv1D(64, 5, activation='relu',\n",
    "            kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # new size is 195\n",
    "    x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(64, 5, activation='relu',\n",
    "            kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # after convolution, size becomes 15 elements long\n",
    "    x = MaxPooling1D(5)(x) # this is the size to globally flatten, 15/5 = 3\n",
    "    # flattened vector max pools across each of the 3 elements\n",
    "    # so vectors is now 192 dimensions 3*64 = 192\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='relu',\n",
    "            kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "                kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    model_cnn_1 = Model(sequence_input, preds)\n",
    "\n",
    "    # if representing as OHE, use categorical_crossentropy\n",
    "    # if representing the class as an integer, use sparse_categorical_crossentropy\n",
    "    model_cnn_1.compile(loss='categorical_crossentropy', \n",
    "                optimizer='rmsprop',\n",
    "                metrics=['Precision'])\n",
    "\n",
    "    print(model_cnn_1.summary())\n",
    "\n",
    "    cnn1_histories = []\n",
    "    tmp = model_cnn_1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "            epochs=30, batch_size=128)\n",
    "    cnn1_histories.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0 points] Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a model by transfer learning from your foundational model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the new model converges. You only need to train a model using the bottleneck features for this step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0 points] Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform fine tuning upon the model by training some layers within the foundational model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the model converges.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.0 points] Report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report the results of all models using the evaluation procedure that you argued for at the beginning of the lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the convergence of the models and the running time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results should be reported with proper statistical comparisons and proper visualizations.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv7324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
