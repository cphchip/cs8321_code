{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAXjym-6H3Zi"
      },
      "source": [
        "# cs8321 Lab 2 - Transfer Learning and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzvDKYIH3Zj"
      },
      "source": [
        "#### Chip Henderson - 48996654"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cZzaCbH3Zj"
      },
      "source": [
        "## [2.0 points] Dataset Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgbHEjGRH3Zk"
      },
      "source": [
        "In this dataset, I'll be working on a sentiment classification. This is a many to one classifier of Amazon reviews. I'll be working with categories of negative, netral, and positive.\n",
        "\n",
        "Thhis version of the Amazon reviews dataset is was updated in 2018 from an original version in 2014. It consists of more than 230 million customer reviews from 1996 to 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOBQmamH3Zk"
      },
      "source": [
        "**What is the feature data? Who collected the data? Why? When? Is it multimodal?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNjO5JzbH3Zk"
      },
      "source": [
        "**What evaluation criteria will you be using, why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l01RLyJTH3Zk"
      },
      "source": [
        "## [2.0 points] Describe the foundational model that you will be using to transfer learn from"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfayZGDCH3Zk"
      },
      "source": [
        "I'll be using the bert-base-uncased model for my foundation model. This model's architecture consists of:\n",
        "* 12 layers\n",
        "* 768 hidden\n",
        "* 12-heads\n",
        "* 110 million parameters\n",
        "\n",
        "and trained on lower-cased English text per the [hugging face repo](https://huggingface.co/transformers/v3.3.1/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UybHNh7H3Zk"
      },
      "source": [
        "**What tasks was the foundational model trained from?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQ_iYspH3Zk"
      },
      "source": [
        "Per the hugging face [blog site](https://huggingface.co/google-bert/bert-base-uncased?text=The+goal+of+a+dog%27s+life+is+%5BMASK%5D), \"the BERT model was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia. It is also known as the Toronto Book Corpus, and consists of the text of around 7,000 self-published books scraped from the indie ebook distribution website Smashwords [per wikipedia](https://en.wikipedia.org/wiki/BookCorpus). The dataset consists of around 985 million words across a large span of genres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLiY3K2BH3Zk"
      },
      "source": [
        "**Explain if the new task is within the same domain, across domains, etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760cTIuaH3Zl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsS5OABH3Zl"
      },
      "source": [
        "## [1.0 points] Split the data into training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkXRV9TjH3Zl"
      },
      "source": [
        "We'll start by importing the data from the source. I'll use a pandas dataframe initially due to its ease of understanding the labels and data types in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsTDbDGqJVFX",
        "outputId": "ed97edf7-11bc-45af-f572-c1a1cc258a38"
      },
      "outputs": [],
      "source": [
        "# Uncomment for use in colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN02bwtkH3Zl",
        "outputId": "0a1be7ac-bf36-4224-ba31-0b5fb86a1602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 1min 52s\n",
            "Wall time: 1min 54s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Source: modified from https://nijianmo.github.io/amazon/index.html for\n",
        "# importing data. Customized path and df name\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "path = \"../Data_sources/Electronics_5.json.gz\" # local\n",
        "# path = \"/content/drive/MyDrive/Colab Notebooks/Data_sources/Electronics_5.json.gz\" # colab\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "init_df = getDF(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "CFplCd0JH3Zm",
        "outputId": "6fb5b770-27f4-421d-b180-1f9ca713777c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>vote</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>67</td>\n",
              "      <td>True</td>\n",
              "      <td>09 18, 1999</td>\n",
              "      <td>AAP7PPBU72QFM</td>\n",
              "      <td>0151004714</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>D. C. Carrad</td>\n",
              "      <td>This is the best novel I have read in 2 or 3 y...</td>\n",
              "      <td>A star is born</td>\n",
              "      <td>937612800</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>10 23, 2013</td>\n",
              "      <td>A2E168DTVGE6SV</td>\n",
              "      <td>0151004714</td>\n",
              "      <td>{'Format:': ' Kindle Edition'}</td>\n",
              "      <td>Evy</td>\n",
              "      <td>Pages and pages of introspection, in the style...</td>\n",
              "      <td>A stream of consciousness novel</td>\n",
              "      <td>1382486400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>09 2, 2008</td>\n",
              "      <td>A1ER5AYS3FQ9O3</td>\n",
              "      <td>0151004714</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Kcorn</td>\n",
              "      <td>This is the kind of novel to read when you hav...</td>\n",
              "      <td>I'm a huge fan of the author and this one did ...</td>\n",
              "      <td>1220313600</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>13</td>\n",
              "      <td>False</td>\n",
              "      <td>09 4, 2000</td>\n",
              "      <td>A1T17LMQABMBN5</td>\n",
              "      <td>0151004714</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Caf Girl Writes</td>\n",
              "      <td>What gorgeous language! What an incredible wri...</td>\n",
              "      <td>The most beautiful book I have ever read!</td>\n",
              "      <td>968025600</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>8</td>\n",
              "      <td>True</td>\n",
              "      <td>02 4, 2000</td>\n",
              "      <td>A3QHJ0FXK33OBE</td>\n",
              "      <td>0151004714</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>W. Shane Schmidt</td>\n",
              "      <td>I was taken in by reviews that compared this b...</td>\n",
              "      <td>A dissenting view--In part.</td>\n",
              "      <td>949622400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
              "0      5.0   67      True  09 18, 1999   AAP7PPBU72QFM  0151004714   \n",
              "1      3.0    5      True  10 23, 2013  A2E168DTVGE6SV  0151004714   \n",
              "2      5.0    4     False   09 2, 2008  A1ER5AYS3FQ9O3  0151004714   \n",
              "3      5.0   13     False   09 4, 2000  A1T17LMQABMBN5  0151004714   \n",
              "4      3.0    8      True   02 4, 2000  A3QHJ0FXK33OBE  0151004714   \n",
              "\n",
              "                            style      reviewerName  \\\n",
              "0       {'Format:': ' Hardcover'}      D. C. Carrad   \n",
              "1  {'Format:': ' Kindle Edition'}               Evy   \n",
              "2       {'Format:': ' Paperback'}             Kcorn   \n",
              "3       {'Format:': ' Hardcover'}   Caf Girl Writes   \n",
              "4       {'Format:': ' Hardcover'}  W. Shane Schmidt   \n",
              "\n",
              "                                          reviewText  \\\n",
              "0  This is the best novel I have read in 2 or 3 y...   \n",
              "1  Pages and pages of introspection, in the style...   \n",
              "2  This is the kind of novel to read when you hav...   \n",
              "3  What gorgeous language! What an incredible wri...   \n",
              "4  I was taken in by reviews that compared this b...   \n",
              "\n",
              "                                             summary  unixReviewTime image  \n",
              "0                                     A star is born       937612800   NaN  \n",
              "1                    A stream of consciousness novel      1382486400   NaN  \n",
              "2  I'm a huge fan of the author and this one did ...      1220313600   NaN  \n",
              "3          The most beautiful book I have ever read!       968025600   NaN  \n",
              "4                        A dissenting view--In part.       949622400   NaN  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "init_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTuEI0kmH3Zm"
      },
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Du-sp65H3Zm"
      },
      "source": [
        "This is a big dataset, and there are a number of columns I don't need. To keep operations faster I'm going to drop everything I don't need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SsGntGwPH3Zm",
        "outputId": "37b458f7-7db3-4f3f-d629-1d3620e29dd1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>This is the best novel I have read in 2 or 3 y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>Pages and pages of introspection, in the style...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>This is the kind of novel to read when you hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>What gorgeous language! What an incredible wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>I was taken in by reviews that compared this b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall                                         reviewText\n",
              "0      5.0  This is the best novel I have read in 2 or 3 y...\n",
              "1      3.0  Pages and pages of introspection, in the style...\n",
              "2      5.0  This is the kind of novel to read when you hav...\n",
              "3      5.0  What gorgeous language! What an incredible wri...\n",
              "4      3.0  I was taken in by reviews that compared this b..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_keep = ['overall','reviewText']\n",
        "features_to_drop = [feature for feature in init_df.columns if feature not in features_to_keep]\n",
        "df = init_df.drop(features_to_drop,axis=1)\n",
        "\n",
        "del init_df # memory management\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joZi_rj9H3Zm",
        "outputId": "f2cb9cc1-dd0c-4470-c712-47926fa9749b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67396 entries, 0 to 67395\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   overall     67396 non-null  float64\n",
            " 1   reviewText  67383 non-null  object \n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df = df.sample(frac=0.01,replace=False) # local\n",
        "# Colab could run 5% of the data but it completely maxed out the 51 GB of RAM\n",
        "# df = df.sample(frac=0.05,replace=False) # colab\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXlbq0HWH3Zm"
      },
      "source": [
        "I'm also going to remove any stop words from the review text. Stop words are words like \"a,” “the,” “is,” “are,\" and don't add a lot of contextual value. So they're a good way to reduce the size of the reviews. Before I can do that I need to make sure there aren't any unrecognized characters so I'll do some additional processing on the review text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OkD6qlDH3Zn",
        "outputId": "7c53c5c6-2a87-4113-d77d-4ff9564ec855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67396 entries, 0 to 67395\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         67396 non-null  float64\n",
            " 1   reviewText      67383 non-null  object \n",
            " 2   str_reviewText  67396 non-null  object \n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "# Note: Built in pandas functionality didn't seem to\n",
        "df['reviewText'] = df['reviewText'].str.strip()\n",
        "# df['str_reviewText'] = df['reviewText'].apply(lambda x: str(x)) # source ChatGPT\n",
        "df['str_reviewText'] = df['reviewText'].astype(str)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZtC6natH3Zn",
        "outputId": "f643ed4c-241d-4ae6-9adb-c8891b5346a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Chip\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Chip\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        I like compact size. I rated 4 stars flash dri...\n",
            "1        This keyboard worth price, I think feels cheap...\n",
            "2        I hooked take long realize I made good choice....\n",
            "3        As extra charging cable fine, feel flimsy iPho...\n",
            "4                  outstanding device, well worth money...\n",
            "                               ...                        \n",
            "67391    Great price get pay for. I system six months d...\n",
            "67392    As much I wanted keep thia bag go. Quality sup...\n",
            "67393    The monitor nice. I bought gaming. The images ...\n",
            "67394    I previous Panasonic blu-ray player I purchase...\n",
            "67395    Very nice portable tripod! I full-sized Manfro...\n",
            "Name: str_reviewText, Length: 67396, dtype: object\n",
            "CPU times: total: 1.91 s\n",
            "Wall time: 2.57 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Use NLTK to remove stopwords\n",
        "import nltk\n",
        "# import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stop_words(review):\n",
        "    # no_tags = re.sub(pattern,'',tweet)\n",
        "    no_stop_words = [word for word in review.split() if word not in stop_words]\n",
        "    return ' '.join(no_stop_words)\n",
        "\n",
        "\n",
        "# Apply the pattern to remove those tags from tweets\n",
        "df['str_reviewText'] = df['str_reviewText'].apply(remove_stop_words)\n",
        "\n",
        "print(df['str_reviewText'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd20GEygH3Zn"
      },
      "source": [
        "Strangely, these don't all seem like electronics reviews...but for my purposes it really doesn't matter. Also, there's no obvious sentiment labels in the dataset. So I'm going to use the overall rating as my sentiments. I'll set up the following categories:\n",
        " -  0-2: Negative\n",
        " -  3: Neutral\n",
        " -  4-5: Positive\n",
        "\n",
        "After grouping, I'll plot the distributions using a pie chart to visually observe how many samples are in each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "PbloInReH3Zn",
        "outputId": "883226dd-6bb6-4960-dd18-b4dc0bf85816"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bracket the sentiments based on Overall value\n",
        "df['sentiment'] = pd.cut(df['overall'], [0,2,3,5], labels=['negative','neutral','positive'])\n",
        "\n",
        "# Group the data by sentiment for purposes of charting\n",
        "sentiments = df.groupby(['sentiment'])\n",
        "pie_data = sentiments.size()\n",
        "\n",
        "# Set the pie chart parameters\n",
        "plt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', labeldistance=1.1, startangle=90)\n",
        "\n",
        "plt.title(\"Quantity of Each Sentiment\")\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Get the exact count of each value to provide further information\n",
        "df.value_counts(['sentiment'])\n",
        "\n",
        "del sentiments # memory management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh1KREOPH3Zn"
      },
      "source": [
        "There is a significant misbalance of my classes, so I'm going to need to make sure I stratify the data so that the data is well represented. Before I do that, I'll tokenize the reviews and and prepare the data for splitting by narrowing down to fields of interest.\n",
        "\n",
        "To get started with tokenizing, I need to know the max length I'll encounter in the reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhGv6ut4H3Zn",
        "outputId": "0a83712c-cb81-4c29-fde6-949d71885571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The longest review is 15879 characters\n"
          ]
        }
      ],
      "source": [
        "# Determine longest review length\n",
        "print(f\"The longest review is {len(max(df['str_reviewText'], key=len))} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgxadxMCH3Zo"
      },
      "source": [
        "In order for all the data going into the model to be same length, I'll need to zero-pad the sequences. For data consistency, I want all reviews to have some zero-padding. If my longest review length is 26,673 characters, I'll round up to 26,680 for my max length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo_BH8ZlH3Zo",
        "outputId": "10c1bc02-6b6e-4105-f8a8-4dba4c5a3a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 52945 unique tokens. Distilled to 52945 top words.\n",
            "Shape of data tensor: (67396, 26680)\n",
            "Shape of label tensor: (67396,)\n",
            "52945\n",
            "CPU times: total: 7.83 s\n",
            "Wall time: 9.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Source: cs7324 lab 7\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "NUM_TOP_WORDS = None # use entire vocabulary!\n",
        "MAX_REVIEW_LEN = 26680  # maximum and minimum number of words\n",
        "\n",
        "#tokenize the text\n",
        "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
        "tokenizer.fit_on_texts(df['str_reviewText'])\n",
        "# save as sequences with integers replacing words\n",
        "sequences = tokenizer.texts_to_sequences(df['str_reviewText'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
        "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
        "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=MAX_REVIEW_LEN)\n",
        "y = df.sentiment.values\n",
        "print('Shape of data tensor:', X.shape)\n",
        "print('Shape of label tensor:', y.shape)\n",
        "print(np.max(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9mnkyW2H3Zo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D9w4zUpmH3Zo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPsnKT3qEB4",
        "outputId": "ad1f51be-2b54-4ec4-90ee-5e1a69009423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data tensor train: (53916, 26680)\n",
            "Shape of data tensor for test: (13480, 26680)\n",
            "Shape of label tensor train: (53916, 3)\n",
            "Shape of label tensor for test: (13480, 3)\n"
          ]
        }
      ],
      "source": [
        "# Source: in class lecture notebook cs7324 13a\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "\n",
        "# Use label encoder to get my data into integer form\n",
        "label_encoder = LabelEncoder()\n",
        "y_enc_train = label_encoder.fit_transform(y_train)\n",
        "y_enc_test = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# One-hot encode the encoded labels\n",
        "y_train_ohe = keras.utils.to_categorical(y_enc_train)\n",
        "y_test_ohe = keras.utils.to_categorical(y_enc_test)\n",
        "\n",
        "# Check the shape of the data and labels to ensure they are correct\n",
        "print('Shape of data tensor train:', X_train.shape)\n",
        "print('Shape of data tensor for test:', X_test.shape)\n",
        "print('Shape of label tensor train:', y_train_ohe.shape)\n",
        "print('Shape of label tensor for test:', y_test_ohe.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lY8WnXVH3Zo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2prjLIV3H3Zo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MhPiRGYH3Zp"
      },
      "source": [
        "**explain how you performed this operation and why you think it is reasonable to split this particular dataset this way**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Fe0vVKH3Zp"
      },
      "source": [
        "**For multi-task datasets, be sure to explain if it is appropriate to stratify within each task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90fmHulH3Zp"
      },
      "source": [
        "**If the dataset is already split for you, explain how the split was achieved and how it is stratified.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWftAZDfH3Zp"
      },
      "source": [
        "## [2.0 points] Train a model from scratch to perform the classification task (this does NOT need to be a transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfsrVV76H3Zp"
      },
      "source": [
        "**Verify the model converges (even if the model is overfit).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5gceFPGH3Zq"
      },
      "source": [
        "#### Convolutional Neural Network 1 (CNN-1)\n",
        "\n",
        "The first CNN I'll run will consist of 64 filters with a width of 5. I'm changing the filter size from the in-class example because my dataset is quite a bit smaller. So my thought being I won't need so many filters to get good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTvq6IlJaFKy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctQgoTDHaEkN",
        "outputId": "d88b350b-b4e0-4911-cb90-3a5fd49f30ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Embedding Shape: (52946, 300) \n",
            " Total words found: 31390 \n",
            " Percentage: 59.28682053412911\n",
            "CPU times: total: 33.5 s\n",
            "Wall time: 53.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Source: Modified from in-class lecture notebook 13a\n",
        "EMBED_SIZE = 300\n",
        "# the embed size should match the file you load glove from\n",
        "embeddings_index = {}\n",
        "f = open(r'../Data_sources/glove.6B.300d.txt')\n",
        "# f = open(r'/content/drive/MyDrive/Colab Notebooks/Data_sources/glove.6B.300d.txt') # colab\n",
        "# save key/array pairs of the embeddings\n",
        "#  the key of the dictionary is the word, the array is the embedding\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# now fill in the matrix, using the ordering from the\n",
        "#  keras word tokenizer from before\n",
        "found_words = 0\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be ALL-ZEROS\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        found_words = found_words+1\n",
        "\n",
        "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
        "      \"Total words found:\",found_words, \"\\n\",\n",
        "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bRd_DzxwjUVU"
      },
      "outputs": [],
      "source": [
        "# Source: Modified from in-class notebook 13a\n",
        "# save this embedding now\n",
        "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBED_SIZE,\n",
        "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
        "                            input_length=MAX_REVIEW_LEN,\n",
        "                            trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ikL1cOUPH3Zq"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "        # Source: Modified from in-class lecture, cs7324, notebook 13a\n",
        "        from tensorflow.keras.metrics import Precision\n",
        "        from tensorflow.keras.models import Model\n",
        "        from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "        from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "\n",
        "        EMBED_SIZE = 300  # same size as loaded from GLOVE\n",
        "        NUM_CLASSES = 3\n",
        "        sequence_input = Input(shape=(MAX_REVIEW_LEN,), dtype='int32')\n",
        "        # starting size: 500\n",
        "        embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
        "        x = Conv1D(64, 5, activation='relu',\n",
        "                kernel_initializer='he_uniform')(embedded_sequences)\n",
        "\n",
        "        # after conv, size becomes: 500-4=496\n",
        "        x = MaxPooling1D(5)(x) # after max pool, 996/5 = 99\n",
        "        x = Dropout(0.2)(x) # after dropout, size is 95\n",
        "        x = Conv1D(64, 5, activation='relu',\n",
        "                kernel_initializer='he_uniform')(x)\n",
        "\n",
        "        # new size is 195\n",
        "        x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv1D(64, 5, activation='relu',\n",
        "                kernel_initializer='he_uniform')(x)\n",
        "\n",
        "        # after convolution, size becomes 15 elements long\n",
        "        x = MaxPooling1D(5)(x) # this is the size to globally flatten, 15/5 = 3\n",
        "        # flattened vector max pools across each of the 3 elements\n",
        "        # so vectors is now 192 dimensions 3*64 = 192\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(64, activation='relu',\n",
        "                kernel_initializer='he_uniform')(x)\n",
        "\n",
        "        preds = Dense(NUM_CLASSES, activation='softmax',\n",
        "                kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "        model_cnn_1 = Model(sequence_input, preds)\n",
        "\n",
        "        # if representing as OHE, use categorical_crossentropy\n",
        "        # if representing the class as an integer, use sparse_categorical_crossentropy\n",
        "        model_cnn_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['Precision'])\n",
        "\n",
        "        print(model_cnn_1.summary())\n",
        "\n",
        "        cnn1_histories = []\n",
        "        tmp = model_cnn_1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
        "                epochs=30, batch_size=128)\n",
        "        cnn1_histories.append(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jr8K0dZmfRz",
        "outputId": "0f800e8a-3c4d-479a-fe20-8215042f9513"
      },
      "outputs": [],
      "source": [
        "# Source: Modified from in class notebook 13a\n",
        "if False: # don't retrain the model\n",
        "    from tensorflow.keras.metrics import Precision\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "    from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "    from tensorflow.keras.layers import Subtract\n",
        "\n",
        "\n",
        "    EMBED_SIZE = 300  # same size as loaded from GLOVE\n",
        "    NUM_CLASSES = 3 # positive, negative, neutral\n",
        "    sequence_input = Input(shape=(MAX_REVIEW_LEN,), dtype='int32')\n",
        "    # starting size: 1000\n",
        "    embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
        "    x = Conv1D(64, 5, activation='relu',\n",
        "            kernel_initializer='he_uniform')(embedded_sequences)\n",
        "\n",
        "    # after conv, size becomes: 1000-4=996\n",
        "    x = MaxPooling1D(5)(x)# after max pool, 996/5 = 199\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(64, 5, activation='relu',\n",
        "            kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    # new size is 195\n",
        "    x = MaxPooling1D(5)(x) # after max pool, size is 195/5 = 39\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(64, 5, activation='relu',\n",
        "            kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    # after convolution, size becomes 15 elements long\n",
        "    # Take the mean of these elements across features, result is 128 elements\n",
        "    x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten\n",
        "\n",
        "    # Take the variance of these elements across features, result is 128 elements\n",
        "    x_tmp = Subtract()([x,x_mean])\n",
        "    x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
        "\n",
        "    x = Concatenate(name='concat_1')([x_mean,x_std])\n",
        "\n",
        "\n",
        "    x = Dense(64, activation='relu',\n",
        "            kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    preds = Dense(NUM_CLASSES, activation='softmax',\n",
        "                kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "    model_xvec = Model(sequence_input, preds)\n",
        "\n",
        "    # if representing as OHE, use categorical_crossentropy\n",
        "    # if representing the class as an integer, use sparse_categorical_crossentropy\n",
        "    model_xvec.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['Precision'])\n",
        "\n",
        "    print(model_xvec.summary())\n",
        "\n",
        "    model_xvec_histories = []\n",
        "    tmp = model_xvec.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
        "            epochs=6, batch_size=128)\n",
        "    model_xvec_histories.append(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "d9VwJVa7aGxg",
        "outputId": "1800e5e8-08d9-47ab-a5be-36a5ce389976"
      },
      "outputs": [],
      "source": [
        "# Source: in class lecture notebook 13a\n",
        "if False: # Don't clear out the diagram\n",
        "    %matplotlib inline\n",
        "\n",
        "    # combine all the history from training together\n",
        "    combined = dict()\n",
        "    for key in ['precision','val_precision','loss','val_loss']:\n",
        "        combined[key] = np.hstack([x.history[key] for x in model_xvec_histories])\n",
        "\n",
        "    # summarize history for precision\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(combined['precision'])\n",
        "    plt.plot(combined['val_precision'])\n",
        "    plt.title('model precision')\n",
        "    plt.ylabel('precision')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.subplot(122)\n",
        "    plt.plot(combined['loss'])\n",
        "    plt.plot(combined['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx-qL71RH3Zq"
      },
      "source": [
        "## [2.0 points] Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128),                                                          \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'sequence_output':  109482241   ['preprocessing[0][0]',          \n",
            "                                 (None, 128, 768),                'preprocessing[0][1]',          \n",
            "                                 'encoder_outputs':               'preprocessing[0][2]']          \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768)}                                                       \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 768)          0           ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            769         ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.6254 - precision: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 1s 959ms/step - loss: 0.7057 - precision: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 1s 969ms/step - loss: 0.7996 - precision: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 1s 960ms/step - loss: 0.8114 - precision: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 1s 961ms/step - loss: 0.8324 - precision: 0.0000e+00\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E8C45C8550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E8C45C8550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[[ 0.6179341 ]\n",
            " [-0.35287112]]\n"
          ]
        }
      ],
      "source": [
        "# Source: modified from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "# Source: Troubleshooting with ChatGPT\n",
        "# Source example gave dictionary of BERT model options, I'm directly selecting one\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Load BERT model from TensorFlow Hub\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "\n",
        "# Define sentiment classification model\n",
        "def build_model():\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(bert_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(bert_model, trainable=False, name='BERT_encoder') # changed trainable from True\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "    return tf.keras.Model(text_input, net)\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['Precision'])\n",
        "\n",
        "# Dummy data for demonstration\n",
        "train_data = [\"I love this movie\", \"This movie is terrible\"]\n",
        "train_labels = [1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
        "            epochs=6, batch_size=128)\n",
        "\n",
        "# Example of sentiment prediction\n",
        "test_data = [\"This movie is great!\", \"I dislike this movie\"]\n",
        "predictions = model.predict(test_data)\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t25vAx_H3Zq"
      },
      "source": [
        "**Train a model by transfer learning from your foundational model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-A4mRB4H3Zq"
      },
      "source": [
        "**Verify that the new model converges. You only need to train a model using the bottleneck features for this step.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C377UiG4H3Zq"
      },
      "source": [
        "## [2.0 points] Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuvIP3-gH3Zr"
      },
      "source": [
        "**Perform fine tuning upon the model by training some layers within the foundational model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGlYaFNlH3Zr"
      },
      "source": [
        "**Verify that the model converges.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH2cAUBCH3Zr"
      },
      "source": [
        "## [4.0 points] Report the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SUC_mSIH3Zr"
      },
      "source": [
        "**Report the results of all models using the evaluation procedure that you argued for at the beginning of the lab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazxUQlUH3Zr"
      },
      "source": [
        "**Compare the convergence of the models and the running time.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-FD2wYiH3Zr"
      },
      "source": [
        "**Results should be reported with proper statistical comparisons and proper visualizations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUCw7rE8H3Zr"
      },
      "source": [
        "## Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1w30SJfZH3Zr"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1616375712.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[60], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://www.tensorflow.org/text/tutorials/classify_text_with_bert\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "https://keras.io/examples/nlp/pretraining_BERT/\n",
        "https://www.smashwords.com/about\n",
        "https://huggingface.co/google-bert/bert-base-uncased?text=The+goal+of+a+dog%27s+life+is+%5BMASK%5D.\n",
        "https://keras.io/guides/keras_nlp/transformer_pretraining/\n",
        "https://huggingface.co/transformers/v3.3.1/pretrained_models.html\n",
        "https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/#:~:text=The%20BERTBase%20model%20uses,has%20around%20110M%20trainable%20parameters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
