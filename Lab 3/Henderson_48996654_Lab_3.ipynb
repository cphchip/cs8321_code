{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs8321 Lab 3 - CNN Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chip Henderson - 48996654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will find and analyze a circuit in a common neural network.  A reference figure is also shown to help clarify the process of finding and analyzing deep circuits.\n",
    "\n",
    "The terminology used in this labs is as follows: (1) a filter refers to the entire tensor that convolves with an input across all channels (i.e., a multi-channel filter like a 3x3x64 tensor), (2) a single channel filter refers to one channel of the aforementioned filter (e.g., a 3x3x1 convolution kernel), (3) and activation refers to the input or output of the filter depending on context (i.e., input activations for a filter are the inputs from a previous layer, output activations are all the filter outputs from a convolutional layer), (4) an input can refer to all input channels or a single input channel depending on the context (entire filter or single channel filter, respectively). A diagram is provided for clarity and this was covered in detail in the class lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3 Points] In groups, you should select a convolutional neural network model that has been pre-trained on a large dataset (preferably, ImageNet). These already trained models are readily available online through many mechanisms, including the keras.application package (Inception, Xception, VGG etc.) https://keras.io/api/applications/Links to an external site.  \n",
    "It is recommended to select a model with somewhat simple structure, like VGG. This can help to simplify how to extract specific filters and inputs to filters later on.  \n",
    " Explain the model you chose and why.  Classify a few images with pre-trained network to verify that it is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull in VGG16 and try to classify some images. I don't have any image datasets locally to I need some representative samples to run through the model. It helps to understand what VGG was trained on in order to do this.\n",
    "\n",
    "VGG-16 was trained on imagenet-1k, also known as ILSVRC 2012. This dataset contains images of varying class and is frequently used to pretrain deep learning models for computer vision. Here are some of the specifications:\n",
    " -  One thousand classes in total\n",
    " -  A datasplit of 1,281,167 training images, 50,000 validation images, and 100,000 test images was used\n",
    " -  The images in the dataset are classified using the WordNet hierarchy\n",
    " -  Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet aims to provide on average 1000 images to illustrate each synset. \n",
    " -  The images of each concept are quality-controlled and human-annotated.\n",
    "\n",
    "VGG19 is another good option for this lab. However, I'm being cognizant of my platform constraints and going with focusing on minimum viable models from a memory footprint perspective.\n",
    "\n",
    "Source: https://paperswithcode.com/dataset/imagenet-1k-1#:~:text=The%20ImageNet1K%20dataset%2C%20also%20known,It%20spans%201000%20object%20classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Source: https://keras.io/api/applications/\n",
    "# Import the model, weights, and set up for classifiying test images\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model imported, we'll pull a couple imageNet images to test performance. The input image size was typically 224x224, so I'll be using images of that same configuration here. I didn't want to download a full imagenet dataset, so I found a source providing one image from each of the 1,000 classes, and took 10 random samples from that.\n",
    "\n",
    "Image Source: https://github.com/EliSchwartz/imagenet-sample-images/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 227ms/step\n",
      "Predictions for n01440764_tench.JPEG\n",
      "('n01440764', 'tench', 0.92435455)\n",
      "('n01443537', 'goldfish', 0.040547475)\n",
      "('n13054560', 'bolete', 0.011226614)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "Predictions for n01498041_stingray.JPEG\n",
      "('n01496331', 'electric_ray', 0.94126046)\n",
      "('n01498041', 'stingray', 0.053825986)\n",
      "('n01910747', 'jellyfish', 0.0019548666)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "Predictions for n01632458_spotted_salamander.JPEG\n",
      "('n01632458', 'spotted_salamander', 0.96983093)\n",
      "('n01629819', 'European_fire_salamander', 0.016018884)\n",
      "('n01630670', 'common_newt', 0.013842737)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "Predictions for n02130308_cheetah.JPEG\n",
      "('n02130308', 'cheetah', 0.918589)\n",
      "('n02127052', 'lynx', 0.07476509)\n",
      "('n02128757', 'snow_leopard', 0.0029449575)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "Predictions for n02174001_rhinoceros_beetle.JPEG\n",
      "('n02174001', 'rhinoceros_beetle', 0.9320676)\n",
      "('n02172182', 'dung_beetle', 0.059250537)\n",
      "('n02167151', 'ground_beetle', 0.0075410255)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "Predictions for n02823428_beer_bottle.JPEG\n",
      "('n02823428', 'beer_bottle', 0.9665323)\n",
      "('n04591713', 'wine_bottle', 0.027124057)\n",
      "('n03109150', 'corkscrew', 0.0029431304)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "Predictions for n03249569_drum.JPEG\n",
      "('n03249569', 'drum', 0.9959281)\n",
      "('n03250847', 'drumstick', 0.0016328966)\n",
      "('n02787622', 'banjo', 0.0012120004)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Predictions for n03950228_pitcher.JPEG\n",
      "('n03950228', 'pitcher', 0.39380226)\n",
      "('n04560804', 'water_jug', 0.3313457)\n",
      "('n02815834', 'beaker', 0.11532847)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "Predictions for n03977966_police_van.JPEG\n",
      "('n03769881', 'minibus', 0.16224146)\n",
      "('n03445924', 'golfcart', 0.11870925)\n",
      "('n03977966', 'police_van', 0.10941496)\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Predictions for n04376876_syringe.JPEG\n",
      "('n04376876', 'syringe', 0.99304295)\n",
      "('n03876231', 'paintbrush', 0.0040425756)\n",
      "('n04154565', 'screwdriver', 0.00050062145)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Source: Modified from https://keras.io/api/applications/\n",
    "# and ChatGPT\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "# These function were provided by ChatGPT and verified against\n",
    "# The Keras applications API https://keras.io/api/applications/\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def predict_image_class(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    preds = model.predict(img)\n",
    "    # Decode predictions, give the top 3\n",
    "    decoded_preds = decode_predictions(preds, top=3)[0]\n",
    "    return decoded_preds\n",
    "\n",
    "images_dir = '../Data_Sources/imageNet_samples/'\n",
    "\n",
    "# Get list of image filenames in the directory\n",
    "image_files = os.listdir(images_dir)\n",
    "\n",
    "# Iterate over each image and make predictions\n",
    "for image_file in image_files:\n",
    "    # print(image_file)\n",
    "    img_path = os.path.join(images_dir, image_file)\n",
    "    predictions = predict_image_class(img_path)\n",
    "    print(\"Predictions for\", image_file)\n",
    "    for prediction in predictions:\n",
    "        print(prediction)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 3\\Henderson_48996654_Lab_3.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m layer_outputs \u001b[39m=\u001b[39m [layer\u001b[39m.\u001b[39moutput \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m:\u001b[39m18\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Creates a model that will return these outputs, given the model input:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m activation_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mModel(inputs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39mlayer_outputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# This will return a list of Numpy arrays:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# one array per layer activation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%203/Henderson_48996654_Lab_3.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m activations \u001b[39m=\u001b[39m activation_model\u001b[39m.\u001b[39mpredict(img_tensor)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'Model'"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import models\n",
    "\n",
    "# Extracts the outputs of the top 18 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[1:18]]\n",
    "\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = model.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# This will return a list of Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)\n",
    "print(len(activations))\n",
    "\n",
    "[print(x.shape) for x in activations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[1:18]: # Layers before fully connected\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "print(layer_names)\n",
    "\n",
    "# images_per_row = 16\n",
    "\n",
    "# # Now let's display our feature maps (the activations variable is from the model.predict above)\n",
    "# for layer_name, layer_activation in zip(layer_names, activations):\n",
    "#     # This is the number of features in the feature map\n",
    "#     n_features = layer_activation.shape[-1]\n",
    "\n",
    "#     # The feature map has shape (1, size, size, n_features)\n",
    "#     size = layer_activation.shape[1]\n",
    "\n",
    "#     # We will tile the activation channels in this matrix\n",
    "#     n_cols = n_features // images_per_row\n",
    "#     display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "#     # We'll tile each filter into this big horizontal grid\n",
    "#     for col in range(n_cols):\n",
    "#         for row in range(images_per_row):\n",
    "#             channel_image = layer_activation[0,\n",
    "#                                              :, :,\n",
    "#                                              col * images_per_row + row]\n",
    "#             # Post-process the feature to make it visually palatable\n",
    "#             channel_image = prepare_image_for_display(channel_image,\n",
    "#                                      norm_type = 'std')\n",
    "#             display_grid[col * size : (col + 1) * size,\n",
    "#                          row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "#     # Display the grid\n",
    "#     scale = 1. / size\n",
    "#     plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "#                         scale * display_grid.shape[0]))\n",
    "#     plt.title(layer_name)\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4 Points] Select a multi-channel filter (i.e., a feature) in a layer in which to analyze as part of a circuit. This should be a multi-channel filter in a \"mid-level\" portion of the network (that is, there are convolutional layers before and after this chosen layer). You might find using OpenAI microscope a helpful tool for selecting a filter to analyze without writing too much code: https://microscope.openai.com/models/Links to an external site. \n",
    "Using image gradient techniques, find an input image that maximally excites this chosen multi-channel filter. General techniques are available from class: https://github.com/8000net/LectureNotesMaster/blob/master/04%20LectureVisualizingConvnets.ipynbLinks to an external site.\n",
    "Also send images of varying class (i.e., from ImageNet) through the network and track which classes of images most excite your chosen filter. \n",
    "Give a hypothesis for what this multi-channel filter might be extracting. That is, what do you think its function is in the network? Give reasoning for your hypothesis. \n",
    "If using code from another source or a LLM, you should heavily document the code so that I can grade your understanding of the code used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4 Points] Analyze each channel of the multi-channel filter to better understand how this might form a circuit (i.e., the weights of the filter). That is, visualize the convolutional filter (one channel at a time) between the input activations and the current activation to understand which inputs make up a circuit. You should avoid filter channels that are mostly \"zero\" coefficients. These are not influential for the circuit. One method of doing this is given below:\n",
    "Extract the filter coefficients for each input activation to that multi-channel filter. Note: If the multi-channel filter is 5x5 with an input channel size of 64, then this extraction will result in 64 different single channel filters, each of size 5x5. \n",
    "Keep the top ten sets of inputs with the \"strongest\" weights. For now, you can use the L2 norm of each input filter as a measure of strength. Visualize these top ten filters. \n",
    "For these ten strongest input filters, categorize each as \"mostly inhibitory\" or \"mostly excitatory.\" That is, does each filter consist of mostly negative or mostly positive coefficients?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4 Points] For each of the ten chosen single channels of the filter, use image gradient techniques to visualize what each of these filters is most excited by (that is, what image maximally excites each of these filters?). This is a similar analysis to the first step in this rubric, but now isolating the activations the layer preceding your chosen filter. This should only be completed for the ten most influential filters.  \n",
    "Use these visualizations, along with the circuit weights you just discovered to try and explain how this particular circuit works. An example of this visualization style can be seen here: https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_379.htmlLinks to an external site. \n",
    "Try to define the properties of this circuit using vocabulary from https://distill.pub/2020/circuits/zoom-in/Links to an external site. (such as determining if this is polysemantic, pose-invariant, etc.)  \n",
    "Relate your visualizations back to your original hypothesis about what this filter is extracting. Does it support or refute your hypothesis? Why? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv7324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
