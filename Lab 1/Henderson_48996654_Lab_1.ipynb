{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1pBn63p31e6R",
      "metadata": {
        "id": "1pBn63p31e6R"
      },
      "source": [
        "# cs8321 Lab 1 - ConceptNet Ethics Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xbjDEuY51e6T",
      "metadata": {
        "id": "xbjDEuY51e6T"
      },
      "source": [
        "#### Chip Henderson - 48996654\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KmfPJxru1e6T",
      "metadata": {
        "id": "KmfPJxru1e6T"
      },
      "outputs": [],
      "source": [
        "# Import everything\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Subtract\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn import metrics as mt\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics as mt\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import seaborn\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043e3052",
      "metadata": {},
      "source": [
        "### Intro\n",
        "\n",
        "In this notebook I'm going to be comparing the results from our in-class discussion and modeling to my results when using a trained transformer. This code is set up to run in google colab so some parts need to be commented out when running on my local system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "UP33fbUA2UJJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP33fbUA2UJJ",
        "outputId": "3c0c0c6f-558c-4791-d283-577a408b004e"
      },
      "outputs": [],
      "source": [
        "# Uncomment for use in colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38c1bea1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 30.9 s\n",
            "Wall time: 37.4 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(516782, 300)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "def load_embeddings(filename):\n",
        "    \"\"\"\n",
        "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
        "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
        "    whether there is an initial line with the dimensions of the matrix.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    rows = []\n",
        "    with open(filename, encoding='utf-8') as infile:\n",
        "        for i, line in enumerate(infile):\n",
        "            items = line.rstrip().split(' ')\n",
        "            if len(items) == 2:\n",
        "                # This is a header row giving the shape of the matrix\n",
        "                continue\n",
        "            labels.append(items[0])\n",
        "            values = np.array([float(x) for x in items[1:]], 'f')\n",
        "            rows.append(values)\n",
        "    \n",
        "    arr = np.vstack(rows)\n",
        "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
        "\n",
        "# embeddings = load_embeddings('data/glove.840B.300d.txt') # course original\n",
        "embeddings = load_embeddings('../Data_Sources/numberbatch-en.txt') # local PC\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "184a4fe4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2006 4783\n"
          ]
        }
      ],
      "source": [
        "# Source: In class notebook 01 ConceptNet\n",
        "def load_lexicon(filename):\n",
        "    \"\"\"\n",
        "    Load a file from Bing Liu's sentiment lexicon\n",
        "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
        "    English words in Latin-1 encoding.\n",
        "\n",
        "    One file contains a list of positive words, and the other contains\n",
        "    a list of negative words. The files contain comment lines starting\n",
        "    with ';' and blank lines, which should be skipped.\n",
        "    \"\"\"\n",
        "    lexicon = []\n",
        "    with open(filename, encoding='latin-1') as infile:\n",
        "        for line in infile:\n",
        "            line = line.rstrip()\n",
        "            if line and not line.startswith(';'):\n",
        "                lexicon.append(line)\n",
        "    return lexicon\n",
        "\n",
        "# For running on local system, use the following:\n",
        "pos_words = load_lexicon('../Data_Sources/positive-words.txt')\n",
        "neg_words = load_lexicon('../Data_Sources/negative-words.txt')\n",
        "\n",
        "# For running in colab, use the following:\n",
        "# pos_words = load_lexicon(r'/content/drive/MyDrive/Colab Notebooks/positive-words.txt') # colab version\n",
        "# neg_words = load_lexicon(r'/content/drive/MyDrive/Colab Notebooks/negative-words.txt') # colab version\n",
        "\n",
        "print(len(pos_words), len(neg_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b2d688ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "NAMES_BY_ETHNICITY = {\n",
        "    # The first two lists are from the Caliskan et al. appendix describing the\n",
        "    # Word Embedding Association Test.\n",
        "    'White': [\n",
        "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
        "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
        "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
        "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
        "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
        "        'Ellen', 'Lauren', 'Peggy', 'Colleen', 'Emily',\n",
        "        'Megan', 'Rachel', 'Wendy'\n",
        "    ],\n",
        "\n",
        "    'Black': [\n",
        "        'Alonzo', 'Jamel', 'Theo', 'Alphonse', 'Jerome',\n",
        "        'Leroy', 'Torrance', 'Darnell', 'Lamar', 'Lionel',\n",
        "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone',\n",
        "        'Lavon', 'Marcellus', 'Wardell', #'Nichelle',\n",
        "        'Ebony', 'Shaniqua',\n",
        "        'Jasmine', 'Tanisha', 'Tia', 'Latoya',\n",
        "        'Yolanda', 'Malika', 'Yvette'\n",
        "    ],\n",
        "    #Larson Edit: had to remove a number of names that were not in the embedding\n",
        "\n",
        "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
        "    'Hispanic': [\n",
        "        'Juan', 'José', 'Miguel', 'Jorge', 'Santiago',\n",
        "        'Mateo',  'Alejandro', 'Samuel', 'Diego', 'Daniel',\n",
        "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Isabella', 'Valentina',\n",
        "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
        "    ],\n",
        "    #Larson Edit: had to remove a number of names that were not in the embedding\n",
        "\n",
        "    # The following list conflates religion and ethnicity, I'm aware. So do given names.\n",
        "    #\n",
        "    # This list was cobbled together from searching baby-name sites for common Muslim names,\n",
        "    # as spelled in English. I did not ultimately distinguish whether the origin of the name\n",
        "    # is Arabic or Urdu or another language.\n",
        "    #\n",
        "    # I'd be happy to replace it with something more authoritative, given a source.\n",
        "    'Arab/Muslim': [\n",
        "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
        "        'Syed', 'Samar', 'Ahmad', 'Rayyan', 'Mariam',\n",
        "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
        "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # print(NAMES_BY_ETHNICITY.values())\n",
        "# full_name_list = []\n",
        "# for list in NAMES_BY_ETHNICITY.values():\n",
        "#     for name in list:\n",
        "#         full_name_list.append(name)\n",
        "\n",
        "# print(len(full_name_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f29baf",
      "metadata": {},
      "source": [
        "Note, vecs_to_sentiment is where the model prediction call is coming from in this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4e2d56a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def vecs_to_sentiment(vecs):\n",
        "    # predict_log_proba gives the log probability for each class\n",
        "    predictions = model.predict_log_proba(vecs)\n",
        "\n",
        "    # To see an overall positive vs. negative classification in one number,\n",
        "    # we take the log probability of positive sentiment minus the log\n",
        "    # probability of negative sentiment.\n",
        "    # this is a logarithm of the max margin for the classifier,\n",
        "    # similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\n",
        "    return predictions[:, 1] - predictions[:, 0]\n",
        "\n",
        "\n",
        "def words_to_sentiment(words):\n",
        "    vecs = embeddings.loc[words].dropna()\n",
        "    log_odds = vecs_to_sentiment(vecs)\n",
        "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
        "\n",
        "\n",
        "def name_sentiment_table():\n",
        "    frames = []\n",
        "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
        "        lower_names = [name.lower() for name in name_list]\n",
        "        sentiments = words_to_sentiment(lower_names)\n",
        "        sentiments['group'] = group\n",
        "        frames.append(sentiments)\n",
        "\n",
        "    # Put together the data we got from each ethnic group into one big table\n",
        "    return pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c5161f58",
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "def retrain_model(new_embs, title = ''):\n",
        "    \"\"\"\n",
        "    Repeat the steps above with a new set of word embeddings.\n",
        "    \"\"\"\n",
        "    # use these from outside the function and update them globally\n",
        "    global model, embeddings, name_sentiments\n",
        "    embeddings = new_embs\n",
        "    # get the positive and negative embeddings\n",
        "    pos_words_common = list(set(pos_words) & set(embeddings.index))\n",
        "    neg_words_common = list(set(neg_words) & set(embeddings.index))\n",
        "    pos_vectors = embeddings.loc[pos_words_common]\n",
        "    neg_vectors = embeddings.loc[neg_words_common]\n",
        "    vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "    targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "    labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
        "\n",
        "    # split the data\n",
        "    train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "        train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
        "\n",
        "    # train our model\n",
        "    model = SGDClassifier(loss='log_loss', random_state=0, max_iter=100)\n",
        "    model.fit(train_vectors, train_targets)\n",
        "\n",
        "    # print out a goodness of fit\n",
        "    accuracy = accuracy_score(model.predict(test_vectors), test_targets)\n",
        "    print(\"Accuracy of sentiment: {:.2%}\".format(accuracy))\n",
        "\n",
        "    # get the name table of different people's names and save embeddings\n",
        "    name_sentiments = name_sentiment_table()\n",
        "\n",
        "    stats.f_oneway(\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'White'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Black'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Hispanic'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Arab/Muslim'],\n",
        "    )\n",
        "\n",
        "    name_sentiments = name_sentiment_table()\n",
        "\n",
        "    fstat,pval = stats.f_oneway(\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'White'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Black'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Hispanic'],\n",
        "        name_sentiments['sentiment'][name_sentiments['group'] == 'Arab/Muslim'],\n",
        "    )\n",
        "    print('F-statistic:',fstat,' With P-value:', pval)\n",
        "\n",
        "    # Show the results on a swarm plot, with a consistent Y-axis\n",
        "    matplotlib.pyplot.figure(figsize=(15,5))\n",
        "    matplotlib.pyplot.subplot(121)\n",
        "    plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments)\n",
        "    plot.set_ylim([-10, 10])\n",
        "    matplotlib.pyplot.subplot(122)\n",
        "    plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)\n",
        "    matplotlib.pyplot.suptitle(title, fontsize=16)\n",
        "\n",
        "    print(fstat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c470e52a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of sentiment: 97.63%\n",
            "F-statistic: 12.332152582324005  With P-value: 4.1596216681950755e-07\n",
            "12.332152582324005\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Local PC version\n",
        "retrain_model(load_embeddings('../Data_Sources/numberbatch-en.txt'), title='ConceptNet Numberbatch')\n",
        "# Colab version\n",
        "# retrain_model(load_embeddings('/content/drive/MyDrive/Colab Notebooks/numberbatch-en.txt'), title='ConceptNet Numberbatch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a838b8b6",
      "metadata": {},
      "source": [
        "### Import and prepare the data for model training\n",
        "\n",
        "In this section of code I'm going to be training a transformer on the same lexicon used in the ConceptNet notebook from class. I'm going to manipulate some of the training data to make it compatible with the transformer. Instead of feeding in the actual word vectors as found in the embedding of choice, I need the words themselves, tokenized and one-hot encoded. They'll also be zero padded to ensure all entries are the same length."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d802cc",
      "metadata": {},
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4c895f3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: Modified from notebook 01 ConceptNet\n",
        "pos_words_common = list(set(pos_words) & set(embeddings.index))\n",
        "neg_words_common = list(set(neg_words) & set(embeddings.index))\n",
        "\n",
        "# I'll need the following to execute the code from class.\n",
        "# But, they won't play a role in my transformer training.\n",
        "pos_vectors = embeddings.loc[pos_words_common]\n",
        "neg_vectors = embeddings.loc[neg_words_common]\n",
        "\n",
        "# For the transformer I'm using the words themslves, not the vectors\n",
        "words = pos_words_common + neg_words_common\n",
        "\n",
        "# This portion is test code\n",
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "\n",
        "'''\n",
        "I'm going to use zero padding to ensure all words are the same length\n",
        "in the transformer. To do this, I need the max length of all the words.\n",
        "Then, I'll add two to it to ensure all words receive padding.\n",
        "'''\n",
        "lengths = [len(x) for x in words]\n",
        "maxlen = 2 + max(lengths) # add two to ensure every word gets padded\n",
        "\n",
        "# Source: ChatGPT to help with conversion from pd dataframe to dataset\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train_vectors, train_targets))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((test_vectors, test_targets))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0fe05c83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: in class notebook 01 ConceptNet\n",
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "labels = list(pos_vectors.index) + list(neg_vectors.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "595ec7e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79dad83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(632,)\n"
          ]
        }
      ],
      "source": [
        "print(test_targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "85d5c00d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data tensor train: (5679, 300)\n",
            "Shape of data tensor for test: (632, 300)\n",
            "Shape of target tensor train: (5679, 2)\n",
            "Shape of target tensor for test: (632, 2)\n"
          ]
        }
      ],
      "source": [
        "# Source: modified from Notebook 13a cs7324\n",
        "\n",
        "# Encode and normalize the target values for the transformer\n",
        "'''\n",
        "I'm not 100% sure the eoncoding of the targets is necessary.\n",
        "They're already integers and maybe I could just skip straight\n",
        "to one-hot-encoding...\n",
        "'''\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# train_targets_enc = label_encoder.fit_transform(train_targets)\n",
        "# test_targets_enc = label_encoder.fit_transform(test_targets)\n",
        "\n",
        "# One hot encode the target values\n",
        "train_targets_ohe = keras.utils.to_categorical(train_targets)\n",
        "test_targets_ohe = keras.utils.to_categorical(test_targets)\n",
        "\n",
        "# Check the shape of the data and labels to ensure they are correct\n",
        "print('Shape of data tensor train:', train_vectors.shape)\n",
        "print('Shape of data tensor for test:', test_vectors.shape)\n",
        "print('Shape of target tensor train:', train_targets_ohe.shape)\n",
        "print('Shape of target tensor for test:', test_targets_ohe.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "d8db6000",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(516782, 300)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedded_sequences.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa074fd5",
      "metadata": {},
      "source": [
        "#### Setting the numberbatch embeddings model as the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "07f64227",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 516783 word vectors.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'word_index' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:20\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "# # Source: Modified from in-class lecture notebook 13a cs7324\n",
        "# EMBED_SIZE = 300\n",
        "# # the embed size should match the file you load numberBatch from\n",
        "# embeddings_index = {}\n",
        "# f = open(r'../Data_Sources/numberbatch-en.txt')\n",
        "# # save key/array pairs of the embeddings\n",
        "# #  the key of the dictionary is the word, the array is the embedding\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # now fill in the matrix, using the ordering from the\n",
        "# #  keras word tokenizer from before\n",
        "# found_words = 0\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be ALL-ZEROS\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#         found_words = found_words+1\n",
        "\n",
        "# print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
        "#       \"Total words found:\",found_words, \"\\n\",\n",
        "#       \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "75e878c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 516783 word vectors.\n",
            "Embedding Shape: (516783, 300) \n",
            " Total words found: 1 \n",
            " Percentage: 0.00019350481730242674\n",
            "CPU times: total: 26.2 s\n",
            "Wall time: 28.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Source: Modified from in-class lecture notebook 13a cs7324\n",
        "EMBED_SIZE = 300\n",
        "# maxlen = 300\n",
        "# the embed size should match the file you load glove from\n",
        "embeddings_index = {}\n",
        "f = open(r'../Data_Sources/numberbatch-en.txt') # local PC version\n",
        "# f = open(r'/content/drive/MyDrive/Colab Notebooks/numberbatch-en.txt') # colab version\n",
        "# save key/array pairs of the embeddings\n",
        "#  the key of the dictionary is the word, the array is the embedding\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# now fill in the matrix, using the ordering from the\n",
        "#  keras word tokenizer from before\n",
        "vocab_size = len(embeddings_index)\n",
        "found_words = 0\n",
        "embedding_matrix = np.zeros((vocab_size, EMBED_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be ALL-ZEROS\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        found_words = found_words+1\n",
        "\n",
        "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
        "      \"Total words found:\",found_words, \"\\n\",\n",
        "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "7f73a355",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'rank'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 1\\Henderson_48996654_Lab_1.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# embedded_sequences = embedding_layer(sequence_input) # from previous embedding\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m embedded_sequences \u001b[39m=\u001b[39m embeddings\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m Conv1D(\u001b[39m64\u001b[39;49m, \u001b[39m5\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m            kernel_initializer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhe_uniform\u001b[39;49m\u001b[39m'\u001b[39;49m)(embedded_sequences)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# after conv, size becomes: 500-4=496\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X64sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m MaxPooling1D(\u001b[39m2\u001b[39m)(x) \u001b[39m# after max pool, 996/5 = 99\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py:251\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    246\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected max_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmax_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         )\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmin_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49mrank\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[0;32m    253\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         )\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
          ]
        }
      ],
      "source": [
        "# Source: Modified from in-class lecture, notebook 13a\n",
        "from tensorflow.keras.metrics import Precision\n",
        "\n",
        "EMBED_SIZE = 300  # same size as loaded from GLOVE\n",
        "NUM_CLASSES = 2\n",
        "sequence_input = Input(shape=(300,), dtype='int32')\n",
        "# embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
        "embedded_sequences = embeddings\n",
        "x = Conv1D(64, 5, activation='relu',\n",
        "           kernel_initializer='he_uniform')(embedded_sequences)\n",
        "\n",
        "# after conv, size becomes: 500-4=496\n",
        "x = MaxPooling1D(2)(x) # after max pool, 996/5 = 99\n",
        "x = Dropout(0.2)(x) # after dropout, size is 95\n",
        "x = Conv1D(64, 5, activation='relu',\n",
        "           kernel_initializer='he_uniform')(x)\n",
        "\n",
        "# new size is 195\n",
        "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19\n",
        "x = Dropout(0.2)(x)\n",
        "x = Conv1D(64, 5, activation='relu',\n",
        "           kernel_initializer='he_uniform')(x)\n",
        "\n",
        "# after convolution, size becomes 15 elements long\n",
        "x = MaxPooling1D(5)(x) # this is the size to globally flatten, 15/5 = 3\n",
        "# flattened vector max pools across each of the 3 elements\n",
        "# so vectors is now 192 dimensions 3*64 = 192\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu',\n",
        "          kernel_initializer='he_uniform')(x)\n",
        "\n",
        "preds = Dense(NUM_CLASSES, activation='softmax',\n",
        "              kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "model_cnn_1 = Model(sequence_input, preds)\n",
        "\n",
        "# if representing as OHE, use categorical_crossentropy\n",
        "# if representing the class as an integer, use sparse_categorical_crossentropy\n",
        "model_cnn_1.compile(loss='categorical_crossentropy', \n",
        "              optimizer='rmsprop',\n",
        "              metrics=['Precision'])\n",
        "\n",
        "print(model_cnn_1.summary())\n",
        "\n",
        "cnn1_histories = []\n",
        "tmp = model_cnn_1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
        "          epochs=30, batch_size=128)\n",
        "cnn1_histories.append(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823a8f20",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe0bab1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1497d95c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41090f1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "17bd4d8e",
      "metadata": {},
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1c6e80ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c6e80ca",
        "outputId": "d3f01276-a9f1-45eb-820e-b96a20d36789"
      },
      "outputs": [],
      "source": [
        "# Source: Modified from notebook 01 ConceptNet\n",
        "pos_words_common = list(set(pos_words) & set(embeddings.index))\n",
        "neg_words_common = list(set(neg_words) & set(embeddings.index))\n",
        "\n",
        "# I'll need the following to execute the code from class.\n",
        "# But, they won't play a role in my transformer training.\n",
        "pos_vectors = embeddings.loc[pos_words_common]\n",
        "neg_vectors = embeddings.loc[neg_words_common]\n",
        "\n",
        "# For the transformer I'm using the words themslves, not the vectors\n",
        "words = pos_words_common + neg_words_common\n",
        "\n",
        "# This portion is test code\n",
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "\n",
        "'''\n",
        "I'm going to use zero padding to ensure all words are the same length\n",
        "in the transformer. To do this, I need the max length of all the words.\n",
        "Then, I'll add two to it to ensure all words receive padding.\n",
        "'''\n",
        "lengths = [len(x) for x in words]\n",
        "maxlen = 2 + max(lengths) # add two to ensure every word gets padded\n",
        "\n",
        "# Source: ChatGPT to help with conversion from pd dataframe to dataset\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train_vectors, train_targets))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((test_vectors, test_targets))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Source: Modified from notebook 01 ConceptNet, cs8321\n",
        "# # get the positive and negative embeddings\n",
        "# pos_words_common = list(set(pos_words) & set(embeddings.index))\n",
        "# neg_words_common = list(set(neg_words) & set(embeddings.index))\n",
        "# pos_vectors = embeddings.loc[pos_words_common]\n",
        "# neg_vectors = embeddings.loc[neg_words_common]\n",
        "# vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "# targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "# labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
        "\n",
        "# # split the data\n",
        "# train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "#     train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
        "\n",
        "# # Source: ChatGPT to help with conversion from pd dataframe to dataset\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train_vectors, train_targets))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((test_vectors, test_targets))\n",
        "\n",
        "# # One-hot encode the target values\n",
        "# train_targets_ohe = keras.utils.to_categorical(train_targets)\n",
        "# test_targets_ohe = keras.utils.to_categorical(test_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464ff713",
      "metadata": {},
      "source": [
        "The process of collecting the targets and labels remains unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "48490dd9",
      "metadata": {
        "id": "48490dd9"
      },
      "outputs": [],
      "source": [
        "# Source: in class notebook 01 ConceptNet\n",
        "# vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
        "labels = list(pos_vectors.index) + list(neg_vectors.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea87e8ac",
      "metadata": {},
      "source": [
        "Below, I'm goign to set up the tokenizing of the lexicon words. This will also apply padding to the words once tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "bd73e721",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd73e721",
        "outputId": "36cea842-2bed-4a17-93bd-58ae692421da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6308 unique tokens. Distilled to 6308 top words.\n",
            "Shape of data tensor: (6311, 20)\n",
            "Shape of label tensor: (6311,)\n",
            "6308\n"
          ]
        }
      ],
      "source": [
        "# Source: Notebook 13a cs7324\n",
        "NUM_TOP_WORDS = None # use entire vocabulary!\n",
        "\n",
        "# Tokenze the text and get integer sequences for each word\n",
        "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
        "tokenizer.fit_on_texts(words)\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "# Note, I'm really not using a top words functionality here. Its a carry over from notebook 13a\n",
        "word_index = tokenizer.word_index\n",
        "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
        "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
        "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
        "\n",
        "# Pad the sequences so they're all identical lengths.\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "y = targets\n",
        "\n",
        "print('Shape of data tensor:', X.shape)\n",
        "print('Shape of label tensor:', y.shape)\n",
        "print(np.max(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "09e69b43",
      "metadata": {
        "id": "09e69b43"
      },
      "outputs": [],
      "source": [
        "# Source: Modified from 01 ConceptNet\n",
        "# Modification: there's really no reason to include the labels in the train_test_split.\n",
        "# They're captured in the targets data and labels are never used again in notebook 01 ConceptNet.\n",
        "train_words, test_words, train_targets, test_targets = train_test_split(X, y, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e6600921",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6600921",
        "outputId": "60af98bd-ffc8-4c9f-8783-f400f679fe29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data tensor train: (5679, 20)\n",
            "Shape of data tensor for test: (632, 20)\n",
            "Shape of target tensor train: (5679, 2)\n",
            "Shape of target tensor for test: (632, 2)\n"
          ]
        }
      ],
      "source": [
        "# Source: modified from Notebook 13a cs7324\n",
        "\n",
        "# Encode and normalize the target values for the transformer\n",
        "'''\n",
        "I'm not 100% sure the eoncoding of the targets is necessary.\n",
        "They're already integers and maybe I could just skip straight\n",
        "to one-hot-encoding...\n",
        "'''\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_targets_enc = label_encoder.fit_transform(train_targets)\n",
        "test_targets_enc = label_encoder.fit_transform(test_targets)\n",
        "\n",
        "# One hot encode the target values\n",
        "train_targets_ohe = keras.utils.to_categorical(train_targets_enc)\n",
        "test_targets_ohe = keras.utils.to_categorical(test_targets_enc)\n",
        "\n",
        "# Check the shape of the data and labels to ensure they are correct\n",
        "print('Shape of data tensor train:', train_words.shape)\n",
        "print('Shape of data tensor for test:', test_words.shape)\n",
        "print('Shape of target tensor train:', train_targets_ohe.shape)\n",
        "print('Shape of target tensor for test:', test_targets_ohe.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe6f6c4",
      "metadata": {},
      "source": [
        "Next, I'll import the embeddings file and set it up as a custom embedding layer for the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "13c73259",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13c73259",
        "outputId": "dbeb266c-0ffc-481f-9f0a-a382be8a71ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 516783 word vectors.\n",
            "Embedding Shape: (6309, 300) \n",
            " Total words found: 6308 \n",
            " Percentage: 99.98414962751625\n",
            "CPU times: total: 26.9 s\n",
            "Wall time: 41.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Source: Modified from in-class lecture notebook 13a cs7324\n",
        "EMBED_SIZE = 300\n",
        "# maxlen = 300\n",
        "# the embed size should match the file you load glove from\n",
        "embeddings_index = {}\n",
        "f = open(r'../Data_Sources/numberbatch-en.txt') # local PC version\n",
        "# f = open(r'/content/drive/MyDrive/Colab Notebooks/numberbatch-en.txt') # colab version\n",
        "# save key/array pairs of the embeddings\n",
        "#  the key of the dictionary is the word, the array is the embedding\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# now fill in the matrix, using the ordering from the\n",
        "#  keras word tokenizer from before\n",
        "found_words = 0\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be ALL-ZEROS\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        found_words = found_words+1\n",
        "\n",
        "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
        "      \"Total words found:\",found_words, \"\\n\",\n",
        "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "AWR2aPlX1e6a",
      "metadata": {
        "id": "AWR2aPlX1e6a"
      },
      "outputs": [],
      "source": [
        "# Source: Modified from in-class notebook 13a cs7324\n",
        "# save this embedding now\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBED_SIZE,\n",
        "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
        "                            input_length=maxlen,\n",
        "                            trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VXyrwMG61e6a",
      "metadata": {
        "id": "VXyrwMG61e6a"
      },
      "source": [
        "### Training the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "L-QVfCUb1e6a",
      "metadata": {
        "id": "L-QVfCUb1e6a"
      },
      "outputs": [],
      "source": [
        "# Source: In-class notebook 13a\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# The transformer architecture\n",
        "class TransformerBlock(Layer): # inherit from Keras Layer\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
        "        super().__init__()\n",
        "        # setup the model heads and feedforward network\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embed_dim)\n",
        "\n",
        "        # make a two layer network that processes the attention\n",
        "        self.ffn = Sequential()\n",
        "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
        "        self.ffn.add( Dense(embed_dim) )\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # apply the layers as needed (similar to PyTorch)\n",
        "\n",
        "        # get the attention output from multi heads\n",
        "        # Using same inpout here is self-attention\n",
        "        # call inputs are (query, value, key)\n",
        "        # if only two inputs given, value and key are assumed the same\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "\n",
        "        # create residual output, with attention\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # apply dropout if training\n",
        "        out1 = self.dropout1(out1, training=training)\n",
        "\n",
        "        # place through feed forward after layer norm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        # apply dropout if training\n",
        "        out2 = self.dropout2(out2, training=training)\n",
        "        #return the residual from Dense layer\n",
        "        return out2\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # create two embeddings\n",
        "        # one for processing the tokens (words)\n",
        "        self.token_emb = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=embed_dim)\n",
        "        # another embedding for processing the position\n",
        "        self.pos_emb = Embedding(input_dim=maxlen,\n",
        "                                 output_dim=embed_dim)\n",
        "    def call(self, x):\n",
        "        # create a static position measure (input)\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
        "        positions = self.pos_emb(positions)# embed these positions\n",
        "        x = self.token_emb(x) # embed the tokens\n",
        "        return x + positions # add embeddngs to get final embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "Alw220P21e6a",
      "metadata": {
        "id": "Alw220P21e6a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Source: In-class lecture notebook 13a\n",
        "# first, let's replace the original embedding in the xformer\n",
        "# with our custom Numberbatch embedding\n",
        "\n",
        "class NbTokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # create two embeddings\n",
        "        # one for processing the tokens, pretrained (words)\n",
        "        self.token_emb = Embedding(len(word_index) + 1,\n",
        "                            EMBED_SIZE,\n",
        "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
        "                            input_length=maxlen,\n",
        "                            trainable=False)\n",
        "\n",
        "        # another embedding for processing the position\n",
        "        self.pos_emb = Embedding(maxlen,\n",
        "                                 EMBED_SIZE,\n",
        "                                 input_length=maxlen,\n",
        "                                 trainable=True\n",
        "                                )\n",
        "\n",
        "    def call(self, x):\n",
        "        # create a static position measure (input)\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)# embed these positions\n",
        "        x = self.token_emb(x) # embed the tokens\n",
        "        return x + positions # add embeddngs to get final embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9daSa6cb1e6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9daSa6cb1e6a",
        "outputId": "f54a1bd8-e439-4f9e-aee7-96a019d77331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " nb_token_and_position_embed  (None, 20, 300)          1898700   \n",
            " ding (NbTokenAndPositionEmb                                     \n",
            " edding)                                                         \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, 20, 300)          1464632   \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 300)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                19264     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,382,726\n",
            "Trainable params: 1,490,026\n",
            "Non-trainable params: 1,892,700\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Source: Modified from in-class lecture notebook 13a\n",
        "\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "inputs = Input(shape=(train_words.shape[1],))\n",
        "x = NbTokenAndPositionEmbedding()(inputs)\n",
        "x = TransformerBlock(EMBED_SIZE, num_heads, ff_dim)(x)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "preds = Dense(NUM_CLASSES, activation='softmax',\n",
        "              kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "model_xformer_nb = Model(inputs=inputs, outputs=preds)\n",
        "print(model_xformer_nb.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "X5eXYK-91e6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5eXYK-91e6a",
        "outputId": "a3c212ca-4594-4d8e-d5b8-bc5d425eab73"
      },
      "outputs": [],
      "source": [
        "# Source: modified from in-class lecture notebook 13a\n",
        "\n",
        "# I don't want to train my model each time I run my notebook\n",
        "# Comment out the following conditional if training is desired\n",
        "if False: \n",
        "    model_xformer_nb.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['Precision']\n",
        "    )\n",
        "    histories_xformer_nb = []\n",
        "    tmp = model_xformer_nb.fit(train_words, train_targets_ohe,\n",
        "                    validation_data=(test_words, test_targets_ohe),\n",
        "                epochs=4, batch_size=16)\n",
        "    histories_xformer_nb.append(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "_BWcjQfu1e6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "_BWcjQfu1e6b",
        "outputId": "fabd4208-c158-4a31-87e5-55cb9ea2354a"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    %matplotlib inline\n",
        "    # Source: in class lecture notebook 13a\n",
        "    # combine all the history from training together\n",
        "    combined = dict()\n",
        "    for key in ['precision','val_precision','loss','val_loss']:\n",
        "        combined[key] = np.hstack([x.history[key] for x in histories_xformer_nb])\n",
        "\n",
        "    # summarize history for precision\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(combined['precision'])\n",
        "    plt.plot(combined['val_precision'])\n",
        "    plt.title('model precision')\n",
        "    plt.ylabel('precision')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.subplot(122)\n",
        "    plt.plot(combined['loss'])\n",
        "    plt.plot(combined['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ARfWj-df0UTL",
      "metadata": {
        "id": "ARfWj-df0UTL"
      },
      "source": [
        "The training on this model looks good. I may not be fully converged, but getting more than 97% precision out of the validation data is probably asking for a bit much. I'll save the weights so I can use them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "GYbIaAWwzfNA",
      "metadata": {
        "id": "GYbIaAWwzfNA"
      },
      "outputs": [],
      "source": [
        "if False: # I also don't want to save off new weights after training\n",
        "    model_xformer_nb.save_weights('bias_xformer_weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b655a7",
      "metadata": {},
      "source": [
        "Let's pull the weights in and try to do the same predictions as done previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f953b22a",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_xformer_nb.load_weights('bias_xformer_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "ababa60c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print out a goodness of fit\n",
        "# Note this isn't working quite right with the xformer\n",
        "# accuracy = accuracy_score(model_xformer_nb.predict(test_words), test_targets_ohe)\n",
        "# print(\"Accuracy of sentiment: {:.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "6ce92cf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Source: Modified from notebook 01 ConceptNet\n",
        "# # Here I need to modify these functions to use my transformer\n",
        "# def vecs_to_sentiment_xformer(vecs):\n",
        "#     # predict_log_proba gives the log probability for each class\n",
        "#     predictions = model_xformer_nb.predict(vecs)\n",
        "\n",
        "#     # To see an overall positive vs. negative classification in one number,\n",
        "#     # we take the log probability of positive sentiment minus the log\n",
        "#     # probability of negative sentiment.\n",
        "#     # this is a logarithm of the max margin for the classifier,\n",
        "#     # similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\n",
        "#     return predictions[:, 1] - predictions[:, 0]\n",
        "\n",
        "\n",
        "# def words_to_sentiment(words):\n",
        "#     vecs = embeddings.loc[words].dropna()\n",
        "#     log_odds = vecs_to_sentiment_xformer(vecs)\n",
        "#     return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
        "\n",
        "\n",
        "# def name_sentiment_table_xformer():\n",
        "#     frames = []\n",
        "#     for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
        "#         lower_names = [name.lower() for name in name_list]\n",
        "#         sentiments = words_to_sentiment(lower_names)\n",
        "#         sentiments['group'] = group\n",
        "#         frames.append(sentiments)\n",
        "\n",
        "#     # Put together the data we got from each ethnic group into one big table\n",
        "#     return pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4af2bfe",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c868b52b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "11b44cad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def vecs_to_sentiment_xformer(vecs):\n",
        "    # predict_log_proba gives the log probability for each class\n",
        "    predictions = model_xformer_nb.predict(vecs)\n",
        "\n",
        "    # To see an overall positive vs. negative classification in one number,\n",
        "    # we take the log probability of positive sentiment minus the log\n",
        "    # probability of negative sentiment.\n",
        "    # this is a logarithm of the max margin for the classifier,\n",
        "    # similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\n",
        "    return predictions[:, 1] - predictions[:, 0]\n",
        "\n",
        "\n",
        "def words_to_sentiment_xformer(words):\n",
        "    # vecs = embeddings.loc[words].dropna()\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    sequences = tokenizer.texts_to_sequences(words)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "    log_odds = vecs_to_sentiment_xformer(padded_sequences)\n",
        "    return pd.DataFrame({'sentiment': log_odds}, index=sequences.index)\n",
        "\n",
        "\n",
        "def name_sentiment_table_xformer():\n",
        "    frames = []\n",
        "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
        "        lower_names = [name.lower() for name in name_list]\n",
        "        sentiments = words_to_sentiment_xformer(lower_names)\n",
        "        sentiments['group'] = group\n",
        "        frames.append(sentiments)\n",
        "\n",
        "    # Put together the data we got from each ethnic group into one big table\n",
        "    return pd.concat(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c952fe9",
      "metadata": {},
      "source": [
        "**Still need to word tokenize the names and 0 pad them**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "4ab9857c",
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'model/nb_token_and_position_embedding/embedding_2/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\2308350163.py\", line 2, in <module>\n      name_sentiments = name_sentiment_table_xformer()\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 26, in name_sentiment_table_xformer\n      sentiments = words_to_sentiment_xformer(lower_names)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 18, in words_to_sentiment_xformer\n      log_odds = vecs_to_sentiment_xformer(padded_sequences)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 3, in vecs_to_sentiment_xformer\n      predictions = model_xformer_nb.predict(vecs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1912540550.py\", line 28, in call\n      x = self.token_emb(x) # embed the tokens\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\core\\embedding.py\", line 272, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/nb_token_and_position_embedding/embedding_2/embedding_lookup'\nindices[6,19] = 6315 is not in [0, 6309)\n\t [[{{node model/nb_token_and_position_embedding/embedding_2/embedding_lookup}}]] [Op:__inference_predict_function_698]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 1\\Henderson_48996654_Lab_1.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# get the name table of different people's names and save embeddings\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m name_sentiments \u001b[39m=\u001b[39m name_sentiment_table_xformer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m stats\u001b[39m.\u001b[39mf_oneway(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     name_sentiments[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m][name_sentiments[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhite\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     name_sentiments[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m][name_sentiments[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBlack\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     name_sentiments[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m][name_sentiments[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHispanic\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     name_sentiments[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m][name_sentiments[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mArab/Muslim\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m name_sentiments \u001b[39m=\u001b[39m name_sentiment_table_xformer()\n",
            "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 1\\Henderson_48996654_Lab_1.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m group, name_list \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(NAMES_BY_ETHNICITY\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     lower_names \u001b[39m=\u001b[39m [name\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m name_list]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     sentiments \u001b[39m=\u001b[39m words_to_sentiment_xformer(lower_names)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     sentiments[\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m group\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     frames\u001b[39m.\u001b[39mappend(sentiments)\n",
            "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 1\\Henderson_48996654_Lab_1.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m padded_sequences \u001b[39m=\u001b[39m pad_sequences(sequences, maxlen\u001b[39m=\u001b[39mmaxlen)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m log_odds \u001b[39m=\u001b[39m vecs_to_sentiment_xformer(padded_sequences)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m: log_odds}, index\u001b[39m=\u001b[39msequences\u001b[39m.\u001b[39mindex)\n",
            "\u001b[1;32mc:\\Users\\Chip\\source\\repos\\cs8321_code\\Lab 1\\Henderson_48996654_Lab_1.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvecs_to_sentiment_xformer\u001b[39m(vecs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# predict_log_proba gives the log probability for each class\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     predictions \u001b[39m=\u001b[39m model_xformer_nb\u001b[39m.\u001b[39;49mpredict(vecs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# To see an overall positive vs. negative classification in one number,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# we take the log probability of positive sentiment minus the log\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# probability of negative sentiment.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# this is a logarithm of the max margin for the classifier,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chip/source/repos/cs8321_code/Lab%201/Henderson_48996654_Lab_1.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions[:, \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m predictions[:, \u001b[39m0\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/nb_token_and_position_embedding/embedding_2/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Chip\\anaconda3\\envs\\mlenv7324\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\2308350163.py\", line 2, in <module>\n      name_sentiments = name_sentiment_table_xformer()\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 26, in name_sentiment_table_xformer\n      sentiments = words_to_sentiment_xformer(lower_names)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 18, in words_to_sentiment_xformer\n      log_odds = vecs_to_sentiment_xformer(padded_sequences)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1993612535.py\", line 3, in vecs_to_sentiment_xformer\n      predictions = model_xformer_nb.predict(vecs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Local\\Temp\\ipykernel_13804\\1912540550.py\", line 28, in call\n      x = self.token_emb(x) # embed the tokens\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Chip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\core\\embedding.py\", line 272, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/nb_token_and_position_embedding/embedding_2/embedding_lookup'\nindices[6,19] = 6315 is not in [0, 6309)\n\t [[{{node model/nb_token_and_position_embedding/embedding_2/embedding_lookup}}]] [Op:__inference_predict_function_698]"
          ]
        }
      ],
      "source": [
        "# get the name table of different people's names and save embeddings\n",
        "name_sentiments = name_sentiment_table_xformer()\n",
        "\n",
        "stats.f_oneway(\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'White'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Black'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Hispanic'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Arab/Muslim'],\n",
        ")\n",
        "\n",
        "name_sentiments = name_sentiment_table_xformer()\n",
        "\n",
        "fstat,pval = stats.f_oneway(\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'White'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Black'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Hispanic'],\n",
        "    name_sentiments['sentiment'][name_sentiments['group'] == 'Arab/Muslim'],\n",
        ")\n",
        "print('F-statistic:',fstat,' With P-value:', pval)\n",
        "\n",
        "# Show the results on a swarm plot, with a consistent Y-axis\n",
        "matplotlib.pyplot.figure(figsize=(15,5))\n",
        "matplotlib.pyplot.subplot(121)\n",
        "plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments)\n",
        "plot.set_ylim([-10, 10])\n",
        "matplotlib.pyplot.subplot(122)\n",
        "plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)\n",
        "matplotlib.pyplot.suptitle(title, fontsize=16)\n",
        "\n",
        "print(fstat)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
